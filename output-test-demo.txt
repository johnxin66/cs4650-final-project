2022-04-24 17:42:05 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2022-04-24 17:42:07 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 5, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.tokenized.de-en', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=True, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_tokens=512, max_tokens_valid=512, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints', save_interval=5, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'data-bin/iwslt14.tokenized.de-en', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': True}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-04-24 17:42:07 | INFO | fairseq.tasks.translation | [de] dictionary: 33552 types
2022-04-24 17:42:07 | INFO | fairseq.tasks.translation | [en] dictionary: 42022 types
cfg
{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 5, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.tokenized.de-en', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=True, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=512, max_tokens_valid=512, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints', save_interval=5, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'data-bin/iwslt14.tokenized.de-en', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': True}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}2022-04-24 17:42:07 | INFO | fairseq_cli.train | loading decoder from language models...
2022-04-24 17:42:07 | INFO | fairseq.file_utils | loading archive file /root/cs4650-final-project/wmt19.en
2022-04-24 17:42:10 | INFO | fairseq.tasks.language_modeling | dictionary: 42022 types
2022-04-24 17:42:14 | INFO | fairseq.models.fairseq_model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 1000, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/checkpoint/nng/tensorboard_logs/2019-04-04/lm_newscrawl_en.mxup1000000.mlr1.0.tmult2.per959000.csn.lrs0.6.wrm16000.int1e-07.nag.lr5e-05.clp0.1.lyr20.hd16.drp0.1.ffn6144.at_d0.1.rl_d0.1.i1024.m1536.o1024.mxtk2048.tps512.seed1.bm=eos.ngpu128', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 128, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://learnfair1082:12597', 'distributed_port': 12597, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 1000000, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': 1e-09, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/checkpoint/nng/lm/wmt/20190404/lm_newscrawl_en.mxup1000000.mlr1.0.tmult2.per959000.csn.lrs0.6.wrm16000.int1e-07.nag.lr5e-05.clp0.1.lyr20.hd16.drp0.1.ffn6144.at_d0.1.rl_d0.1.i1024.m1536.o1024.mxtk2048.tps512.seed1.bm=eos.ngpu128', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 959000, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 128}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm_gbw', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.1, 'decoder_embed_dim': 1536, 'decoder_output_dim': 1024, 'decoder_input_dim': 1024, 'decoder_ffn_embed_dim': 6144, 'decoder_layers': 20, 'decoder_attention_heads': 16, 'decoder_normalize_before': True, 'no_decoder_final_norm': True, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': False, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': 512, 'tpu': True}, 'task': {'_name': 'language_modeling', 'data': '/root/cs4650-final-project/wmt19.en', 'sample_break_mode': 'eos', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': 512, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': True, 'use_plasma_view': True, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'nag', 'momentum': 0.99, 'weight_decay': 0.0, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'cosine', 'warmup_updates': 16000, 'warmup_init_lr': 1e-07, 'lr': [5e-05], 'min_lr': 0.0, 't_mult': 2.0, 'lr_period_updates': 959000.0, 'lr_shrink': 0.6, 'max_update': 1000000}, 'scoring': None, 'bpe': {'_name': 'fastbpe', 'bpe_codes': '/root/cs4650-final-project/wmt19.en/bpecodes'}, 'tokenizer': {'_name': 'moses', 'source_lang': 'en', 'target_lang': 'en', 'moses_no_dash_splits': False, 'moses_no_escape': False}, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
Loading codes from /root/cs4650-final-project/wmt19.en/bpecodes ...
Read 30000 codes from the codes file.

no_encoder_attn True
en_lm.decoder.layers
ModuleList(
  (0): TransformerDecoderLayerBase(
    (dropout_module): FairseqDropout()
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
    )
    (activation_dropout_module): FairseqDropout()
    (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
    (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  )
  (1): TransformerDecoderLayerBase(
    (dropout_module): FairseqDropout()
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
    )
    (activation_dropout_module): FairseqDropout()
    (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
    (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  )
  (2): TransformerDecoderLayerBase(
    (dropout_module): FairseqDropout()
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
    )
    (activation_dropout_module): FairseqDropout()
    (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
    (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  )
  (3): TransformerDecoderLayerBase(
    (dropout_module): FairseqDropout()
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
    )
    (activation_dropout_module): FairseqDropout()
    (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
    (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  )
  (4): TransformerDecoderLayerBase(
    (dropout_module): FairseqDropout()
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
    )
    (activation_dropout_module): FairseqDropout()
    (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
    (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  )
  (5): TransformerDecoderLayerBase(
    (dropout_module): FairseqDropout()
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
    )
    (activation_dropout_module): FairseqDropout()
    (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
    (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  )
  (6): TransformerDecoderLayerBase(
    (dropout_module): FairseqDropout()
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
    )
    (activation_dropout_module): FairseqDropout()
    (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
    (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  )
  (7): TransformerDecoderLayerBase(
    (dropout_module): FairseqDropout()
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
    )
    (activation_dropout_module): FairseqDropout()
    (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
    (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  )
  (8): TransformerDecoderLayerBase(
    (dropout_module): FairseqDropout()
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
    )
    (activation_dropout_module): FairseqDropout()
    (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
    (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  )
  (9): TransformerDecoderLayerBase(
    (dropout_module): FairseqDropout()
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
    )
    (activation_dropout_module): FairseqDropout()
    (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
    (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  )
  (10): TransformerDecoderLayerBase(
    (dropout_module): FairseqDropout()
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
    )
    (activation_dropout_module): FairseqDropout()
    (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
    (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  )
  (11): TransformerDecoderLayerBase(
    (dropout_module): FairseqDropout()
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
    )
    (activation_dropout_module): FairseqDropout()
    (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
    (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  )
  (12): TransformerDecoderLayerBase(
    (dropout_module): FairseqDropout()
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
    )
    (activation_dropout_module): FairseqDropout()
    (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
    (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  )
  (13): TransformerDecoderLayerBase(
    (dropout_module): FairseqDropout()
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
    )
    (activation_dropout_module): FairseqDropout()
    (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
    (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  )
  (14): TransformerDecoderLayerBase(
    (dropout_module): FairseqDropout()
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
    )
    (activation_dropout_module): FairseqDropout()
    (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
    (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  )
  (15): TransformerDecoderLayerBase(
    (dropout_module): FairseqDropout()
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
    )
    (activation_dropout_module): FairseqDropout()
    (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
    (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  )
  (16): TransformerDecoderLayerBase(
    (dropout_module): FairseqDropout()
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
    )
    (activation_dropout_module): FairseqDropout()
    (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
    (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  )
  (17): TransformerDecoderLayerBase(
    (dropout_module): FairseqDropout()
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
    )
    (activation_dropout_module): FairseqDropout()
    (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
    (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  )
  (18): TransformerDecoderLayerBase(
    (dropout_module): FairseqDropout()
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
    )
    (activation_dropout_module): FairseqDropout()
    (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
    (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  )
  (19): TransformerDecoderLayerBase(
    (dropout_module): FairseqDropout()
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
    )
    (activation_dropout_module): FairseqDropout()
    (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
    (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  )
)2022-04-24 17:42:17 | INFO | fairseq_cli.train | Swapped decoder from language model
2022-04-24 17:42:17 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(33552, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(42022, 1024, padding_idx=1)
    (project_in_dim): Linear(in_features=1024, out_features=1536, bias=False)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1536, out_features=6144, bias=True)
        (fc2): Linear(in_features=6144, out_features=1536, bias=True)
        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=1536, bias=True)
          (v_proj): Linear(in_features=512, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1536, out_features=6144, bias=True)
        (fc2): Linear(in_features=6144, out_features=1536, bias=True)
        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=1536, bias=True)
          (v_proj): Linear(in_features=512, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1536, out_features=6144, bias=True)
        (fc2): Linear(in_features=6144, out_features=1536, bias=True)
        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=1536, bias=True)
          (v_proj): Linear(in_features=512, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1536, out_features=6144, bias=True)
        (fc2): Linear(in_features=6144, out_features=1536, bias=True)
        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=1536, bias=True)
          (v_proj): Linear(in_features=512, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1536, out_features=6144, bias=True)
        (fc2): Linear(in_features=6144, out_features=1536, bias=True)
        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=1536, bias=True)
          (v_proj): Linear(in_features=512, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1536, out_features=6144, bias=True)
        (fc2): Linear(in_features=6144, out_features=1536, bias=True)
        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=1536, bias=True)
          (v_proj): Linear(in_features=512, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1536, out_features=6144, bias=True)
        (fc2): Linear(in_features=6144, out_features=1536, bias=True)
        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=1536, bias=True)
          (v_proj): Linear(in_features=512, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1536, out_features=6144, bias=True)
        (fc2): Linear(in_features=6144, out_features=1536, bias=True)
        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=1536, bias=True)
          (v_proj): Linear(in_features=512, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1536, out_features=6144, bias=True)
        (fc2): Linear(in_features=6144, out_features=1536, bias=True)
        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=1536, bias=True)
          (v_proj): Linear(in_features=512, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1536, out_features=6144, bias=True)
        (fc2): Linear(in_features=6144, out_features=1536, bias=True)
        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=1536, bias=True)
          (v_proj): Linear(in_features=512, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1536, out_features=6144, bias=True)
        (fc2): Linear(in_features=6144, out_features=1536, bias=True)
        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=1536, bias=True)
          (v_proj): Linear(in_features=512, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1536, out_features=6144, bias=True)
        (fc2): Linear(in_features=6144, out_features=1536, bias=True)
        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=1536, bias=True)
          (v_proj): Linear(in_features=512, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
      (12): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1536, out_features=6144, bias=True)
        (fc2): Linear(in_features=6144, out_features=1536, bias=True)
        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=1536, bias=True)
          (v_proj): Linear(in_features=512, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
      (13): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1536, out_features=6144, bias=True)
        (fc2): Linear(in_features=6144, out_features=1536, bias=True)
        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=1536, bias=True)
          (v_proj): Linear(in_features=512, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
      (14): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1536, out_features=6144, bias=True)
        (fc2): Linear(in_features=6144, out_features=1536, bias=True)
        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=1536, bias=True)
          (v_proj): Linear(in_features=512, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
      (15): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1536, out_features=6144, bias=True)
        (fc2): Linear(in_features=6144, out_features=1536, bias=True)
        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=1536, bias=True)
          (v_proj): Linear(in_features=512, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
      (16): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1536, out_features=6144, bias=True)
        (fc2): Linear(in_features=6144, out_features=1536, bias=True)
        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=1536, bias=True)
          (v_proj): Linear(in_features=512, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
      (17): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1536, out_features=6144, bias=True)
        (fc2): Linear(in_features=6144, out_features=1536, bias=True)
        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=1536, bias=True)
          (v_proj): Linear(in_features=512, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
      (18): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1536, out_features=6144, bias=True)
        (fc2): Linear(in_features=6144, out_features=1536, bias=True)
        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=1536, bias=True)
          (v_proj): Linear(in_features=512, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
      (19): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1536, out_features=6144, bias=True)
        (fc2): Linear(in_features=6144, out_features=1536, bias=True)
        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=1536, bias=True)
          (v_proj): Linear(in_features=512, out_features=1536, bias=True)
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (project_out_dim): Linear(in_features=1536, out_features=1024, bias=False)
    (output_projection): Linear(in_features=1024, out_features=42022, bias=False)
  )
)
2022-04-24 17:42:17 | INFO | fairseq_cli.train | task: TranslationTask
2022-04-24 17:42:17 | INFO | fairseq_cli.train | model: TransformerModel
2022-04-24 17:42:17 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-04-24 17:42:17 | INFO | fairseq_cli.train | num. shared model params: 811,645,952 (num. trained: 811,645,952)
2022-04-24 17:42:17 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-04-24 17:42:17 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.de
2022-04-24 17:42:17 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.en
2022-04-24 17:42:17 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en valid de-en 7283 examples
2022-04-24 17:42:17 | INFO | fairseq.trainer | detected shared parameter: decoder.project_in_dim.bias <- decoder.project_out_dim.bias
2022-04-24 17:42:17 | INFO | fairseq.trainer | detected shared parameter: decoder.project_in_dim.bias <- decoder.output_projection.bias
2022-04-24 17:42:17 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-04-24 17:42:17 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 15.740 GB ; name = NVIDIA RTX A4000                        
2022-04-24 17:42:17 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-04-24 17:42:17 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-04-24 17:42:17 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-04-24 17:42:17 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/checkpoint_last.pt
2022-04-24 17:42:20 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2022-04-24 17:42:22 | INFO | fairseq.trainer | Loaded checkpoint checkpoints/checkpoint_last.pt (epoch 11 @ 104340 updates)
2022-04-24 17:42:22 | INFO | fairseq.trainer | loading train data for epoch 11
2022-04-24 17:42:22 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.tokenized.de-en/train.de-en.de
2022-04-24 17:42:22 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.tokenized.de-en/train.de-en.en
2022-04-24 17:42:22 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en train de-en 160239 examples
2022-04-24 17:42:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 10434
2022-04-24 17:42:22 | INFO | fairseq.trainer | begin training epoch 11
2022-04-24 17:42:22 | INFO | fairseq_cli.train | Start iterating over samples

/root/cs4650-final-project/fairseq/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2022-04-24 17:42:43 | INFO | train_inner | epoch 011:     60 / 10434 loss=3.872, nll_loss=2.132, ppl=4.38, wps=1142.4, ups=3.03, wpb=378, bsz=16.2, num_updates=104400, lr=9.787e-05, gnorm=15.153, train_wall=20, gb_free=3.5, wall=26
2022-04-24 17:43:16 | INFO | train_inner | epoch 011:    160 / 10434 loss=3.808, nll_loss=2.054, ppl=4.15, wps=1118.7, ups=2.98, wpb=375.2, bsz=13.8, num_updates=104500, lr=9.78232e-05, gnorm=13.344, train_wall=32, gb_free=3.5, wall=59
2022-04-24 17:43:51 | INFO | train_inner | epoch 011:    260 / 10434 loss=3.888, nll_loss=2.15, ppl=4.44, wps=1115.5, ups=2.89, wpb=386.4, bsz=16.6, num_updates=104600, lr=9.77764e-05, gnorm=14.592, train_wall=34, gb_free=3.5, wall=94
2022-04-24 17:44:26 | INFO | train_inner | epoch 011:    360 / 10434 loss=3.892, nll_loss=2.153, ppl=4.45, wps=1077.1, ups=2.84, wpb=379.3, bsz=15.7, num_updates=104700, lr=9.77297e-05, gnorm=13.43, train_wall=34, gb_free=3.5, wall=129
2022-04-24 17:45:01 | INFO | train_inner | epoch 011:    460 / 10434 loss=3.808, nll_loss=2.057, ppl=4.16, wps=1027, ups=2.85, wpb=360.7, bsz=13.6, num_updates=104800, lr=9.76831e-05, gnorm=14.693, train_wall=34, gb_free=3.5, wall=164
2022-04-24 17:45:37 | INFO | train_inner | epoch 011:    560 / 10434 loss=3.849, nll_loss=2.102, ppl=4.29, wps=1050.1, ups=2.85, wpb=368.7, bsz=14.6, num_updates=104900, lr=9.76365e-05, gnorm=12.831, train_wall=34, gb_free=3.5, wall=199
2022-04-24 17:46:12 | INFO | train_inner | epoch 011:    660 / 10434 loss=3.873, nll_loss=2.134, ppl=4.39, wps=1061.2, ups=2.82, wpb=376.3, bsz=14.4, num_updates=105000, lr=9.759e-05, gnorm=13.23, train_wall=34, gb_free=3.5, wall=235
2022-04-24 17:46:47 | INFO | train_inner | epoch 011:    760 / 10434 loss=3.873, nll_loss=2.13, ppl=4.38, wps=1037.8, ups=2.85, wpb=363.5, bsz=15.6, num_updates=105100, lr=9.75436e-05, gnorm=12.844, train_wall=34, gb_free=3.5, wall=270
2022-04-24 17:47:22 | INFO | train_inner | epoch 011:    860 / 10434 loss=3.881, nll_loss=2.144, ppl=4.42, wps=1049.6, ups=2.86, wpb=367.4, bsz=14.3, num_updates=105200, lr=9.74972e-05, gnorm=15.509, train_wall=34, gb_free=3.5, wall=305
2022-04-24 17:47:57 | INFO | train_inner | epoch 011:    960 / 10434 loss=3.953, nll_loss=2.225, ppl=4.68, wps=1094.8, ups=2.82, wpb=387.8, bsz=16.2, num_updates=105300, lr=9.74509e-05, gnorm=13.796, train_wall=34, gb_free=3.5, wall=340
2022-04-24 17:48:32 | INFO | train_inner | epoch 011:   1060 / 10434 loss=3.937, nll_loss=2.208, ppl=4.62, wps=1057.2, ups=2.87, wpb=368.7, bsz=15, num_updates=105400, lr=9.74047e-05, gnorm=15.821, train_wall=34, gb_free=3.5, wall=375
2022-04-24 17:49:07 | INFO | train_inner | epoch 011:   1160 / 10434 loss=3.909, nll_loss=2.172, ppl=4.51, wps=1053.7, ups=2.88, wpb=366.2, bsz=15.2, num_updates=105500, lr=9.73585e-05, gnorm=15.243, train_wall=34, gb_free=3.5, wall=410
2022-04-24 17:49:42 | INFO | train_inner | epoch 011:   1260 / 10434 loss=3.867, nll_loss=2.121, ppl=4.35, wps=1058.7, ups=2.86, wpb=370.8, bsz=13.4, num_updates=105600, lr=9.73124e-05, gnorm=13.83, train_wall=34, gb_free=3.5, wall=445
2022-04-24 17:50:17 | INFO | train_inner | epoch 011:   1360 / 10434 loss=3.973, nll_loss=2.249, ppl=4.75, wps=1082.2, ups=2.84, wpb=380.9, bsz=15.5, num_updates=105700, lr=9.72663e-05, gnorm=14.881, train_wall=34, gb_free=3.5, wall=480
2022-04-24 17:50:52 | INFO | train_inner | epoch 011:   1460 / 10434 loss=3.958, nll_loss=2.231, ppl=4.69, wps=1057.6, ups=2.87, wpb=369.1, bsz=15.5, num_updates=105800, lr=9.72203e-05, gnorm=13.636, train_wall=34, gb_free=3.5, wall=515
2022-04-24 17:51:27 | INFO | train_inner | epoch 011:   1560 / 10434 loss=3.936, nll_loss=2.205, ppl=4.61, wps=1084.2, ups=2.84, wpb=381.2, bsz=13.9, num_updates=105900, lr=9.71744e-05, gnorm=13.456, train_wall=34, gb_free=3.5, wall=550
2022-04-24 17:52:02 | INFO | train_inner | epoch 011:   1660 / 10434 loss=3.946, nll_loss=2.218, ppl=4.65, wps=1084.2, ups=2.85, wpb=380.5, bsz=16.1, num_updates=106000, lr=9.71286e-05, gnorm=13.407, train_wall=34, gb_free=3.5, wall=585
2022-04-24 17:52:37 | INFO | train_inner | epoch 011:   1760 / 10434 loss=3.996, nll_loss=2.273, ppl=4.83, wps=1065.3, ups=2.85, wpb=373.3, bsz=15.4, num_updates=106100, lr=9.70828e-05, gnorm=18.228, train_wall=34, gb_free=3.5, wall=620
2022-04-24 17:53:13 | INFO | train_inner | epoch 011:   1860 / 10434 loss=4.004, nll_loss=2.285, ppl=4.87, wps=1090.6, ups=2.85, wpb=382.8, bsz=14.5, num_updates=106200, lr=9.70371e-05, gnorm=16.294, train_wall=34, gb_free=3.5, wall=655
2022-04-24 17:53:47 | INFO | train_inner | epoch 011:   1960 / 10434 loss=3.955, nll_loss=2.226, ppl=4.68, wps=1064.9, ups=2.88, wpb=370.3, bsz=13.7, num_updates=106300, lr=9.69914e-05, gnorm=13.757, train_wall=34, gb_free=3.5, wall=690
2022-04-24 17:54:22 | INFO | train_inner | epoch 011:   2060 / 10434 loss=3.937, nll_loss=2.204, ppl=4.61, wps=1075.8, ups=2.85, wpb=377.8, bsz=14.6, num_updates=106400, lr=9.69458e-05, gnorm=14.542, train_wall=34, gb_free=3.5, wall=725
2022-04-24 17:54:58 | INFO | train_inner | epoch 011:   2160 / 10434 loss=4.045, nll_loss=2.334, ppl=5.04, wps=1079, ups=2.84, wpb=380, bsz=18.7, num_updates=106500, lr=9.69003e-05, gnorm=13.108, train_wall=34, gb_free=3.5, wall=760
2022-04-24 17:55:33 | INFO | train_inner | epoch 011:   2260 / 10434 loss=4.009, nll_loss=2.289, ppl=4.89, wps=1102.4, ups=2.82, wpb=390.6, bsz=14.2, num_updates=106600, lr=9.68549e-05, gnorm=14.906, train_wall=34, gb_free=3.5, wall=796
2022-04-24 17:56:08 | INFO | train_inner | epoch 011:   2360 / 10434 loss=3.923, nll_loss=2.19, ppl=4.56, wps=1037.2, ups=2.88, wpb=359.6, bsz=14.4, num_updates=106700, lr=9.68095e-05, gnorm=13.613, train_wall=34, gb_free=3.5, wall=831
2022-04-24 17:56:43 | INFO | train_inner | epoch 011:   2460 / 10434 loss=3.953, nll_loss=2.224, ppl=4.67, wps=1062.8, ups=2.86, wpb=371.6, bsz=14.8, num_updates=106800, lr=9.67641e-05, gnorm=25.382, train_wall=34, gb_free=3.5, wall=866
2022-04-24 17:57:18 | INFO | train_inner | epoch 011:   2560 / 10434 loss=3.947, nll_loss=2.218, ppl=4.65, wps=1066.4, ups=2.86, wpb=372.6, bsz=13.6, num_updates=106900, lr=9.67189e-05, gnorm=14.39, train_wall=34, gb_free=3.5, wall=900
2022-04-24 17:57:53 | INFO | train_inner | epoch 011:   2660 / 10434 loss=4.015, nll_loss=2.301, ppl=4.93, wps=1093.7, ups=2.85, wpb=384.3, bsz=16.4, num_updates=107000, lr=9.66736e-05, gnorm=16.866, train_wall=34, gb_free=3.5, wall=936
2022-04-24 17:58:28 | INFO | train_inner | epoch 011:   2760 / 10434 loss=4.01, nll_loss=2.292, ppl=4.9, wps=1063.9, ups=2.86, wpb=372, bsz=16.3, num_updates=107100, lr=9.66285e-05, gnorm=15.989, train_wall=34, gb_free=3.5, wall=971
2022-04-24 17:59:03 | INFO | train_inner | epoch 011:   2860 / 10434 loss=4.038, nll_loss=2.323, ppl=5, wps=1083.7, ups=2.84, wpb=381.9, bsz=15, num_updates=107200, lr=9.65834e-05, gnorm=14.689, train_wall=34, gb_free=3.5, wall=1006
2022-04-24 17:59:38 | INFO | train_inner | epoch 011:   2960 / 10434 loss=4.036, nll_loss=2.322, ppl=5, wps=1088.2, ups=2.83, wpb=384.3, bsz=15.1, num_updates=107300, lr=9.65384e-05, gnorm=15.731, train_wall=34, gb_free=3.5, wall=1041
2022-04-24 18:00:13 | INFO | train_inner | epoch 011:   3060 / 10434 loss=3.983, nll_loss=2.26, ppl=4.79, wps=1066.2, ups=2.85, wpb=374.2, bsz=14.3, num_updates=107400, lr=9.64935e-05, gnorm=13.66, train_wall=34, gb_free=3.5, wall=1076
2022-04-24 18:00:49 | INFO | train_inner | epoch 011:   3160 / 10434 loss=4.058, nll_loss=2.348, ppl=5.09, wps=1066.2, ups=2.85, wpb=374.1, bsz=17.7, num_updates=107500, lr=9.64486e-05, gnorm=13.144, train_wall=34, gb_free=3.5, wall=1111
2022-04-24 18:01:23 | INFO | train_inner | epoch 011:   3260 / 10434 loss=3.966, nll_loss=2.238, ppl=4.72, wps=1034.2, ups=2.88, wpb=359.5, bsz=14.1, num_updates=107600, lr=9.64037e-05, gnorm=17.195, train_wall=34, gb_free=3.5, wall=1146
2022-04-24 18:01:59 | INFO | train_inner | epoch 011:   3360 / 10434 loss=4.072, nll_loss=2.363, ppl=5.15, wps=1094.9, ups=2.83, wpb=386.4, bsz=14.7, num_updates=107700, lr=9.6359e-05, gnorm=15.059, train_wall=34, gb_free=3.5, wall=1181
Traceback (most recent call last):
  File "train.py", line 593, in <module>
    cli_main()
  File "train.py", line 586, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/root/cs4650-final-project/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "train.py", line 218, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/opt/conda/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "train.py", line 344, in train
    log_output = trainer.train_step(samples)
  File "/opt/conda/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/root/cs4650-final-project/fairseq/fairseq/trainer.py", line 1066, in train_step
    logging_output = self._reduce_and_log_stats(
  File "/root/cs4650-final-project/fairseq/fairseq/trainer.py", line 1490, in _reduce_and_log_stats
    if grad_norm is not None and (
KeyboardInterrupt
