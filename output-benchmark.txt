2022-04-23 20:50:27 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2022-04-23 20:50:28 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.tokenized.de-en', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=True, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'data-bin/iwslt14.tokenized.de-en', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': True}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-04-23 20:50:28 | INFO | fairseq.tasks.translation | [de] dictionary: 33552 types
2022-04-23 20:50:28 | INFO | fairseq.tasks.translation | [en] dictionary: 42022 types
2022-04-23 20:50:29 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(33552, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(42022, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=42022, bias=False)
  )
)
2022-04-23 20:50:29 | INFO | fairseq_cli.train | task: TranslationTask
2022-04-23 20:50:29 | INFO | fairseq_cli.train | model: TransformerModel
2022-04-23 20:50:29 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-04-23 20:50:29 | INFO | fairseq_cli.train | num. shared model params: 70,237,184 (num. trained: 70,237,184)
2022-04-23 20:50:29 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-04-23 20:50:29 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.de
2022-04-23 20:50:29 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.en
2022-04-23 20:50:29 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en valid de-en 7283 examples
2022-04-23 20:50:31 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-04-23 20:50:31 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-04-23 20:50:31 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 15.740 GB ; name = NVIDIA RTX A4000                        
2022-04-23 20:50:31 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-04-23 20:50:31 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-04-23 20:50:31 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None
2022-04-23 20:50:31 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/checkpoint_last.pt
2022-04-23 20:50:31 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/checkpoint_last.pt
2022-04-23 20:50:31 | INFO | fairseq.trainer | loading train data for epoch 1
2022-04-23 20:50:31 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.tokenized.de-en/train.de-en.de
2022-04-23 20:50:31 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.tokenized.de-en/train.de-en.en
2022-04-23 20:50:31 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en train de-en 160239 examples
2022-04-23 20:50:31 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2022-04-23 20:50:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2022-04-23 20:50:31 | INFO | fairseq.trainer | begin training epoch 1
2022-04-23 20:50:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-04-23 20:50:45 | INFO | train_inner | epoch 001:    100 / 1102 loss=14.56, nll_loss=14.388, ppl=21444.5, wps=25911.7, ups=7.37, wpb=3507.8, bsz=121.4, num_updates=100, lr=1.25e-05, gnorm=3.823, train_wall=14, gb_free=11.9, wall=14
2022-04-23 20:50:59 | INFO | train_inner | epoch 001:    200 / 1102 loss=12.493, nll_loss=12.082, ppl=4335.83, wps=25670.4, ups=7.2, wpb=3563.2, bsz=141.4, num_updates=200, lr=2.5e-05, gnorm=1.782, train_wall=14, gb_free=11.8, wall=28
2022-04-23 20:51:13 | INFO | train_inner | epoch 001:    300 / 1102 loss=10.977, nll_loss=10.351, ppl=1305.96, wps=25169.3, ups=7.05, wpb=3572.2, bsz=133.4, num_updates=300, lr=3.75e-05, gnorm=1.664, train_wall=14, gb_free=11.8, wall=42
2022-04-23 20:51:28 | INFO | train_inner | epoch 001:    400 / 1102 loss=9.923, nll_loss=9.077, ppl=540.11, wps=24249.6, ups=6.61, wpb=3671.1, bsz=158.7, num_updates=400, lr=5e-05, gnorm=1.63, train_wall=15, gb_free=11.7, wall=57
2022-04-23 20:51:43 | INFO | train_inner | epoch 001:    500 / 1102 loss=9.709, nll_loss=8.775, ppl=438.17, wps=24309, ups=6.67, wpb=3646.3, bsz=152.7, num_updates=500, lr=6.25e-05, gnorm=1.631, train_wall=15, gb_free=12.1, wall=72
2022-04-23 20:51:58 | INFO | train_inner | epoch 001:    600 / 1102 loss=9.504, nll_loss=8.529, ppl=369.47, wps=24062.6, ups=6.71, wpb=3585.1, bsz=156.3, num_updates=600, lr=7.5e-05, gnorm=1.494, train_wall=15, gb_free=12, wall=87
2022-04-23 20:52:13 | INFO | train_inner | epoch 001:    700 / 1102 loss=9.277, nll_loss=8.27, ppl=308.63, wps=23960.6, ups=6.7, wpb=3576.4, bsz=163.6, num_updates=700, lr=8.75e-05, gnorm=1.733, train_wall=15, gb_free=11.7, wall=102
2022-04-23 20:52:28 | INFO | train_inner | epoch 001:    800 / 1102 loss=9.196, nll_loss=8.18, ppl=289.96, wps=24198, ups=6.74, wpb=3590.6, bsz=137.3, num_updates=800, lr=0.0001, gnorm=1.676, train_wall=15, gb_free=11.8, wall=117
2022-04-23 20:52:43 | INFO | train_inner | epoch 001:    900 / 1102 loss=9.017, nll_loss=7.98, ppl=252.45, wps=24022.1, ups=6.77, wpb=3547.8, bsz=139.6, num_updates=900, lr=0.0001125, gnorm=1.6, train_wall=15, gb_free=11.9, wall=132
2022-04-23 20:52:58 | INFO | train_inner | epoch 001:   1000 / 1102 loss=8.828, nll_loss=7.766, ppl=217.74, wps=24190.4, ups=6.73, wpb=3594, bsz=153.3, num_updates=1000, lr=0.000125, gnorm=1.666, train_wall=15, gb_free=11.9, wall=147
2022-04-23 20:53:12 | INFO | train_inner | epoch 001:   1100 / 1102 loss=8.728, nll_loss=7.655, ppl=201.55, wps=24225.3, ups=6.8, wpb=3561.2, bsz=142, num_updates=1100, lr=0.0001375, gnorm=1.771, train_wall=15, gb_free=11.7, wall=161
2022-04-23 20:53:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
/root/cs4650-final-project/fairseq/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2022-04-23 20:53:13 | INFO | fairseq.tasks.translation | example hypothesis: they don't don't know.
2022-04-23 20:53:13 | INFO | fairseq.tasks.translation | example reference: they're just not moving.
2022-04-23 20:53:14 | INFO | fairseq.tasks.translation | example hypothesis: and what said, "i said," i said? ""
2022-04-23 20:53:14 | INFO | fairseq.tasks.translation | example reference: i was like "what?"
2022-04-23 20:53:15 | INFO | fairseq.tasks.translation | example hypothesis: this is this?
2022-04-23 20:53:15 | INFO | fairseq.tasks.translation | example reference: is it all worked out? no.
2022-04-23 20:53:16 | INFO | fairseq.tasks.translation | example hypothesis: we need to do to do we need to do we need to do to do.
2022-04-23 20:53:16 | INFO | fairseq.tasks.translation | example reference: we need to use less to make more.
2022-04-23 20:53:17 | INFO | fairseq.tasks.translation | example hypothesis: but::::::: UNKNOWNTOKENINHYP: UNKNOWNTOKENINHYP: UNKNOWNTOKENINHYP: UNKNOWNTOKENINHYP: but but UNKNOWNTOKENINHYP: UNKNOWNTOKENINHYP: UNKNOWNTOKENINHYP.
2022-04-23 20:53:17 | INFO | fairseq.tasks.translation | example reference: but homage to singUNKNOWNTOKENINREF re.
2022-04-23 20:53:17 | INFO | fairseq.tasks.translation | example hypothesis: we're "" "we're" "" "we're" "." "" ""
2022-04-23 20:53:17 | INFO | fairseq.tasks.translation | example reference: we call this UNKNOWNTOKENINREF tational lensing.
2022-04-23 20:53:18 | INFO | fairseq.tasks.translation | example hypothesis: now, if you know, when you think of the UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP.
2022-04-23 20:53:18 | INFO | fairseq.tasks.translation | example reference: well, consider by UNKNOWNTOKENINREF y, the concept of physical health.
2022-04-23 20:53:19 | INFO | fairseq.tasks.translation | example hypothesis: these are the UNKNOWNTOKENINHYP are the UNKNOWNTOKENINHYP.
2022-04-23 20:53:19 | INFO | fairseq.tasks.translation | example reference: these are the classic conditions that create regret.
2022-04-23 20:53:20 | INFO | fairseq.tasks.translation | example hypothesis: or or or or or or or or or or or or or or or or or or or or or or or or or or or or
2022-04-23 20:53:20 | INFO | fairseq.tasks.translation | example reference: valence means good or bad, positive or negative.
2022-04-23 20:53:21 | INFO | fairseq.tasks.translation | example hypothesis: it's the UNKNOWNTOKENINHYP is that it's the way that it's the UNKNOWNTOKENINHYP.
2022-04-23 20:53:21 | INFO | fairseq.tasks.translation | example reference: the bad news is they're rocket fuels.
2022-04-23 20:53:22 | INFO | fairseq.tasks.translation | example hypothesis: you can see it's a lot of the UNKNOWNTOKENINHYP.
2022-04-23 20:53:22 | INFO | fairseq.tasks.translation | example reference: it can be used for calculating UNKNOWNTOKENINREF equations of all different types.
2022-04-23 20:53:22 | INFO | fairseq.tasks.translation | example hypothesis: and we're going to do what we're going to do, and we're going to do.
2022-04-23 20:53:22 | INFO | fairseq.tasks.translation | example reference: we should probably slow down, and that point of action is probably now.
2022-04-23 20:53:23 | INFO | fairseq.tasks.translation | example hypothesis: and we know, we have the UNKNOWNTOKENINHYP, and we have the UNKNOWNTOKENINHYP, and we know, and we know, and we're going to get the UNKNOWNTOKENINHYP
2022-04-23 20:53:23 | INFO | fairseq.tasks.translation | example reference: decided to use UNKNOWNTOKENINREF ed content from cement and steel manufacturing.
2022-04-23 20:53:24 | INFO | fairseq.tasks.translation | example hypothesis: we have in the UNKNOWNTOKENINHYP, in the UNKNOWNTOKENINHYP, in the UNKNOWNTOKENINHYP.
2022-04-23 20:53:24 | INFO | fairseq.tasks.translation | example reference: just in the last two days, we got the new temperature UNKNOWNTOKENINREF ds in UNKNOWNTOKENINREF ary.
2022-04-23 20:53:25 | INFO | fairseq.tasks.translation | example hypothesis: this is a lot of this thing that this is that that that that's a lot of this is a lot of this thing that that's a lot of this
2022-04-23 20:53:25 | INFO | fairseq.tasks.translation | example reference: and to remind you that here is an example in which architecture actually did something.
2022-04-23 20:53:26 | INFO | fairseq.tasks.translation | example hypothesis: this was a lot of the UNKNOWNTOKENINHYP that that that that that was that that that was that that that that that was a lot of the time.
2022-04-23 20:53:26 | INFO | fairseq.tasks.translation | example reference: and the grandmother had never let any westerners ever see her.
2022-04-23 20:53:27 | INFO | fairseq.tasks.translation | example hypothesis: so, we don't know this is not not not not not not not not not not not not not not not not not not not not not not not not not that
2022-04-23 20:53:27 | INFO | fairseq.tasks.translation | example reference: this is something we've actually known for a while, so it's not a real breakthrough.
2022-04-23 20:53:28 | INFO | fairseq.tasks.translation | example hypothesis: if it's a lot of it's a lot of it's a lot of it's a UNKNOWNTOKENINHYP, but it's a lot of it's a lot of the way.
2022-04-23 20:53:28 | INFO | fairseq.tasks.translation | example reference: it's a social awkwardness like you're a stranger in a foreign land.
2022-04-23 20:53:29 | INFO | fairseq.tasks.translation | example hypothesis: and i was to say, i was a UNKNOWNTOKENINHYP, i was a UNKNOWNTOKENINHYP, i was a UNKNOWNTOKENINHYP, i was a UNKNOWNTOKENINHYP, and i was going to say, i was to
2022-04-23 20:53:29 | INFO | fairseq.tasks.translation | example reference: i had learned to read music by then, or slowly learning to read music.
2022-04-23 20:53:30 | INFO | fairseq.tasks.translation | example hypothesis: if we don't know, if you know, if you know, if we don't know, if you know, if we're going to do it's going to do it
2022-04-23 20:53:30 | INFO | fairseq.tasks.translation | example reference: they said they were going to beat us up if we didn't make some gunpowder for them.
2022-04-23 20:53:30 | INFO | fairseq.tasks.translation | example hypothesis: now, we know, we know, we know, we know, we know, we know, we know, we know, we know, we know of the UNKNOWNTOKENINHYP of the UNKNOWNTOKENINHYP
2022-04-23 20:53:30 | INFO | fairseq.tasks.translation | example reference: well, we collect data from satellites, from airplanes, from ground UNKNOWNTOKENINREF cles, from people.
2022-04-23 20:53:31 | INFO | fairseq.tasks.translation | example hypothesis: and this was the world of the UNKNOWNTOKENINHYP, and the UNKNOWNTOKENINHYP, and the world was the UNKNOWNTOKENINHYP, and the UNKNOWNTOKENINHYP, and the UNKNOWNTOKENINHYP, and the UNKNOWNTOKENINHYP, and the world was the UNKNOWNTOKENINHYP
2022-04-23 20:53:31 | INFO | fairseq.tasks.translation | example reference: and one of the UNKNOWNTOKENINREF things that happened was this act of actually connecting to all these people.
2022-04-23 20:53:32 | INFO | fairseq.tasks.translation | example hypothesis: i'm going to get a lot of a lot of a lot of a lot of a lot of a lot of a lot of a lot.
2022-04-23 20:53:32 | INFO | fairseq.tasks.translation | example reference: i want to share one story about an innovation called drip irriUNKNOWNTOKENINREF.
2022-04-23 20:53:33 | INFO | fairseq.tasks.translation | example hypothesis: and that was a UNKNOWNTOKENINHYP, and that was, that was, and he was, that was, and he was, and he was, and he was, and he was, and he was,
2022-04-23 20:53:33 | INFO | fairseq.tasks.translation | example reference: the man, his name was mahmoud, and the child, whose name was rafi, left.
2022-04-23 20:53:34 | INFO | fairseq.tasks.translation | example hypothesis: and this is the world, and that you know, and you know, and you know, and you know, and you know, and you know, and you know, and you know, and you
2022-04-23 20:53:34 | INFO | fairseq.tasks.translation | example reference: and that allows me to check my timer discretely, here, without bending my elbow.
2022-04-23 20:53:35 | INFO | fairseq.tasks.translation | example hypothesis: this is a lot of the UNKNOWNTOKENINHYP, in the UNKNOWNTOKENINHYP, in the UNKNOWNTOKENINHYP, in the UNKNOWNTOKENINHYP, in the UNKNOWNTOKENINHYP, in the UNKNOWNTOKENINHYP, in the UNKNOWNTOKENINHYP, in the UNKNOWNTOKENINHYP, in the UNKNOWNTOKENINHYP, in the
2022-04-23 20:53:35 | INFO | fairseq.tasks.translation | example reference: she arrived at our reserve from a reserve east of us on her UNKNOWNTOKENINREF atory route.
2022-04-23 20:53:36 | INFO | fairseq.tasks.translation | example hypothesis: i think that we think that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that we can see that
2022-04-23 20:53:36 | INFO | fairseq.tasks.translation | example reference: if we can make it through the next 150 years, i think that your great great grandchildren will forget all about malthus.
2022-04-23 20:53:37 | INFO | fairseq.tasks.translation | example hypothesis: so, you know, you know, you know, you know, you know, you know, and you know, and you know, and you can see the UNKNOWNTOKENINHYP, and you know, and you know,
2022-04-23 20:53:37 | INFO | fairseq.tasks.translation | example reference: those are then selected by the UNKNOWNTOKENINREF m, reproduced with mutation and reUNKNOWNTOKENINREF ations to introduce sex as well.
2022-04-23 20:53:38 | INFO | fairseq.tasks.translation | example hypothesis: but it's not not not not not, but but it's not not not not not not not not not not not not not not not not not not not not not not not not not not not a UNKNOWNTOKENINHYP.
2022-04-23 20:53:38 | INFO | fairseq.tasks.translation | example reference: now, it just shouldn't take these sorts of efforts to find out where the money in deals like this went.
2022-04-23 20:53:39 | INFO | fairseq.tasks.translation | example hypothesis: if you don't know, if if you don't know, or if you're not not not not not not not not not not not not not not not not not not not not not not not not not not not not the
2022-04-23 20:53:39 | INFO | fairseq.tasks.translation | example reference: when people don't take their pills, when people don't follow doctors' orders -- these are behavior problems.
2022-04-23 20:53:39 | INFO | fairseq.tasks.translation | example hypothesis: if you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, it's the world, it's the world, you know,
2022-04-23 20:53:39 | INFO | fairseq.tasks.translation | example reference: so it's pretty sad to find that UNKNOWNTOKENINREF es, like so many other UNKNOWNTOKENINREF around the world, are losing their habitats.
2022-04-23 20:53:40 | INFO | fairseq.tasks.translation | example hypothesis: and we have a lot of that we have a lot of that we know that we have a lot of that we have a lot of that we have a lot of that we have a lot of that we know that we have a lot of
2022-04-23 20:53:40 | INFO | fairseq.tasks.translation | example reference: we discovered bluegrass a few years ago, and we fell in love with it. we hope you guys will too.
2022-04-23 20:53:41 | INFO | fairseq.tasks.translation | example hypothesis: so, i think what we can do?
2022-04-23 20:53:41 | INFO | fairseq.tasks.translation | example reference: so i was trying to, you know, take the engineer's version: can we build a mechanical system in inorganic materials that will do the same thing?
2022-04-23 20:53:42 | INFO | fairseq.tasks.translation | example hypothesis: we have a lot of the UNKNOWNTOKENINHYP, we have a lot of UNKNOWNTOKENINHYP, we have a lot of the UNKNOWNTOKENINHYP, we have a lot of the UNKNOWNTOKENINHYP, we have a lot of the UNKNOWNTOKENINHYP, we have a lot of UNKNOWNTOKENINHYP, we have to
2022-04-23 20:53:42 | INFO | fairseq.tasks.translation | example reference: we consult to the media about canopy questions; we have a canopy newsletter; we have an email listserv.
2022-04-23 20:53:43 | INFO | fairseq.tasks.translation | example hypothesis: and it's a lot of the way, and it's going to be a UNKNOWNTOKENINHYP, and it's a lot of the UNKNOWNTOKENINHYP, and it's going to be the UNKNOWNTOKENINHYP, and it's going to be a UNKNOWNTOKENINHYP, and it's a UNKNOWNTOKENINHYP
2022-04-23 20:53:43 | INFO | fairseq.tasks.translation | example reference: what my purpose of the talk today really is, is to sort of indelibly scar your minds with these charismatic and majestic UNKNOWNTOKENINREF.
2022-04-23 20:53:44 | INFO | fairseq.tasks.translation | example hypothesis: if you know, if you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know
2022-04-23 20:53:44 | INFO | fairseq.tasks.translation | example reference: now, it's no surprise that when you add consecutive fibonacci numbers, you get the next fibonacci number. right?
2022-04-23 20:53:45 | INFO | fairseq.tasks.translation | example hypothesis: but we know, but we know, but we know, but we know, but we know, but we know, but we know, but we know, but we know, but we know, but we know, but we know, but we know, but
2022-04-23 20:53:45 | INFO | fairseq.tasks.translation | example reference: yet we dither, taking no action to deflect the UNKNOWNTOKENINREF id, even though the longer we wait, the more difficult and expensive it becomes. "
2022-04-23 20:53:46 | INFO | fairseq.tasks.translation | example hypothesis: he said, he was a very very very UNKNOWNTOKENINHYP, he was a very very very very very very very very very very very very very very very very very very very very very very UNKNOWNTOKENINHYP, he was a very very very very very very very very very very very UNKNOWNTOKENINHYP,
2022-04-23 20:53:46 | INFO | fairseq.tasks.translation | example reference: he had this amazing series of hairs growing out of a mole on the left side of his face, which i'm told is very good luck.
2022-04-23 20:53:47 | INFO | fairseq.tasks.translation | example hypothesis: and this is a lot of a lot of the UNKNOWNTOKENINHYP, a UNKNOWNTOKENINHYP, and the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, a UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP,
2022-04-23 20:53:47 | INFO | fairseq.tasks.translation | example reference: here's the first battery -- a stack of coins, zinc and silver, separated by cardboard soaked in brine.
2022-04-23 20:53:48 | INFO | fairseq.tasks.translation | example hypothesis: this is the UNKNOWNTOKENINHYP of the UNKNOWNTOKENINHYP, that that that's a UNKNOWNTOKENINHYP of the UNKNOWNTOKENINHYP of the UNKNOWNTOKENINHYP of the UNKNOWNTOKENINHYP, in the UNKNOWNTOKENINHYP of the UNKNOWNTOKENINHYP, that that's the UNKNOWNTOKENINHYP, that's the UNKNOWNTOKENINHYP, that's the UNKNOWNTOKENINHYP, that's the UNKNOWNTOKENINHYP, that's the UNKNOWNTOKENINHYP
2022-04-23 20:53:48 | INFO | fairseq.tasks.translation | example reference: durkheim called this level the level of the sacred because he believed that the function of religion was to unite people into a group, into a moral community.
2022-04-23 20:53:49 | INFO | fairseq.tasks.translation | example hypothesis: but when when i think, when when when i think, if if if if if i think, when i think, when i think, when i think, but i think, but i think, when i think, but i think the UNKNOWNTOKENINHYP, but i think, but i think, if
2022-04-23 20:53:49 | INFO | fairseq.tasks.translation | example reference: but the UNKNOWNTOKENINREF match up if i rotate by a sixth of a turn around the point where all the trianUNKNOWNTOKENINREF meet.
2022-04-23 20:53:50 | INFO | fairseq.tasks.translation | example hypothesis: and if you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, and you know, you know,
2022-04-23 20:53:50 | INFO | fairseq.tasks.translation | example reference: there's little brochures all around outside, and if any of you have anything to do with children and care about their future, i beg that you pick up that brochure.
2022-04-23 20:53:51 | INFO | fairseq.tasks.translation | example hypothesis: this is a lot of the UNKNOWNTOKENINHYP, and the UNKNOWNTOKENINHYP, and the UNKNOWNTOKENINHYP, and the UNKNOWNTOKENINHYP, in the UNKNOWNTOKENINHYP, and the UNKNOWNTOKENINHYP, and the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, and the UNKNOWNTOKENINHYP, and the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, and the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP
2022-04-23 20:53:51 | INFO | fairseq.tasks.translation | example reference: and this is only done in four hours, 50 times faster than the current state of the art, at a cost that will be five to 500 times cheaper than the current options.
2022-04-23 20:53:52 | INFO | fairseq.tasks.translation | example hypothesis: and i think, what i think, what you can see, what you can see, and you know, and you know, what you know, and you know, what you can see, and you can see, what you know, what you can see, and you can see, and you know,
2022-04-23 20:53:52 | INFO | fairseq.tasks.translation | example reference: i'm fascinated with the idea of what happens when you merge UNKNOWNTOKENINREF with technology, and i remember reading about this idea of being able to reprogram UNKNOWNTOKENINREF, in the future, away from disease and aging.
2022-04-23 20:53:53 | INFO | fairseq.tasks.translation | example hypothesis: but you know, you know, you know, but you know, but you know, you know, but you know, but you know, but you know, but you know, but you know, but you know, but you know, but you know, but you know, but you know, but you know,
2022-04-23 20:53:53 | INFO | fairseq.tasks.translation | example reference: but the problem with relying on rules and inUNKNOWNTOKENINREF ves is that they demorUNKNOWNTOKENINREF professional activity, and they demorUNKNOWNTOKENINREF professional activity in two UNKNOWNTOKENINREF.
2022-04-23 20:53:54 | INFO | fairseq.tasks.translation | example hypothesis: i said, "" i said, "" "i said," i said, "i said," i said, "i said," i said, "i said," i said, "i said," i said, "i said," i said, "i said," i said,
2022-04-23 20:53:54 | INFO | fairseq.tasks.translation | example reference: i belong to an internet discussion forum, an UNKNOWNTOKENINREF internet discussion forum, and i asked them, i said, "since 1960, we've had exactly 204 UNKNOWNTOKENINREF heads of state, since 1960."
2022-04-23 20:53:55 | INFO | fairseq.tasks.translation | example hypothesis: and i think that i think that i think, and i think that i think that i think that i'm going to do a very very very very very very very very very very very very very very very very very very very very very UNKNOWNTOKENINHYP, and i think, and i think that i think that i think that i think that i think
2022-04-23 20:53:55 | INFO | fairseq.tasks.translation | example reference: and as these dark clouds were circling me, and i was finding it really, really difficult to think of anything good, i said to myself that i really needed a way to focus on the positive somehow.
2022-04-23 20:53:56 | INFO | fairseq.tasks.translation | example hypothesis: you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you
2022-04-23 20:53:56 | INFO | fairseq.tasks.translation | example reference: for example, gUNKNOWNTOKENINREF age discharge, something you would think just simply goes away, but the laws regulating ship discharge of gUNKNOWNTOKENINREF age actually get weaker the further you are from shore.
2022-04-23 20:53:57 | INFO | fairseq.tasks.translation | example hypothesis: so, he was a lot of the UNKNOWNTOKENINHYP, and he said, and he was a UNKNOWNTOKENINHYP, and he said, and he was a lot of the UNKNOWNTOKENINHYP, and he said, and he said, and he said, and he said, and he said, and he said, and he said, and he said, and he said, and he said
2022-04-23 20:53:57 | INFO | fairseq.tasks.translation | example reference: see, he owned a UNKNOWNTOKENINREF mium plating company, and they had to move heavy steel parts between tanks of chemicals, and so he needed an industrial robot like this that could basically do the heavy lifting.
2022-04-23 20:53:58 | INFO | fairseq.tasks.translation | example hypothesis: the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP
2022-04-23 20:53:58 | INFO | fairseq.tasks.translation | example reference: and the religious police imposes the supposed UNKNOWNTOKENINREF ic way of life on every citizen, by force -- like women are forced to cover their heads -- wear the hijab, the UNKNOWNTOKENINREF ic head cover.
2022-04-23 20:53:59 | INFO | fairseq.tasks.translation | example hypothesis: and the world of the world, the world, the world, in the world, the world, the world, the world, the world, the world of the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world
2022-04-23 20:53:59 | INFO | fairseq.tasks.translation | example reference: come with me to the bottom of the world, antUNKNOWNTOKENINREF tica, the highest, driest, windiest, and yes, coldest region on earth -- more arid than the UNKNOWNTOKENINREF a and, in parts, colder than UNKNOWNTOKENINREF.
2022-04-23 20:54:00 | INFO | fairseq.tasks.translation | example hypothesis: here's here here here here here here here here's here here here here here here here here here here here here here's here here here here here here here here here's a little little little little little little little little little UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP UNKNOWNTOKENINHYP, here here here here here here here here here here here here here here here here here here here here here here's here here here's a UNKNOWNTOKENINHYP
2022-04-23 20:54:00 | INFO | fairseq.tasks.translation | example reference: that's kellar autumn, my former ph.d. student, professor now at lewis and clark, literally giving his firUNKNOWNTOKENINREF born child up for this test.
2022-04-23 20:54:01 | INFO | fairseq.tasks.translation | example hypothesis: and it's a lot of the way that we think that we think of the way, and it's the way, and it's the way, and it's the way, and it's the way, and it's the way, and it's the way, and it's the way, and it's the way, and it's the way, and it's the way, and it's,
2022-04-23 20:54:01 | INFO | fairseq.tasks.translation | example reference: however, while it's easier to think of us, the citizens, the police, the army, as the good guys, and them, the narcos, the carteles, as the bad guys, if you think about it, the latter are only providing a service to the former.
2022-04-23 20:54:02 | INFO | fairseq.tasks.translation | example hypothesis: and i think that i think, in the UNKNOWNTOKENINHYP, i think that i think that i think, in the UNKNOWNTOKENINHYP, in the UNKNOWNTOKENINHYP, and i think that i think of the UNKNOWNTOKENINHYP, in the UNKNOWNTOKENINHYP, and i think of the UNKNOWNTOKENINHYP, and i think of the UNKNOWNTOKENINHYP, and i think that i think of the UNKNOWNTOKENINHYP, in the UNKNOWNTOKENINHYP, and i think of the UNKNOWNTOKENINHYP, and i think of the UNKNOWNTOKENINHYP, and
2022-04-23 20:54:02 | INFO | fairseq.tasks.translation | example reference: and for the following 25 years, living in italy, living in UNKNOWNTOKENINREF, i doled out a piece of this romance to anybody who'd pay for it -- this sense, this aesthetic feeling, for the experience revolving around a designed object.
2022-04-23 20:54:03 | INFO | fairseq.tasks.translation | example hypothesis: so, we have a lot of the UNKNOWNTOKENINHYP, but we're not not not not not in the UNKNOWNTOKENINHYP, but we're not not not not not not not not not not not not not a UNKNOWNTOKENINHYP, but that we know, but we know, but that we have a UNKNOWNTOKENINHYP, but we know, but we have a UNKNOWNTOKENINHYP, but we know, but we don't know, but we know, but we don't know, but
2022-04-23 20:54:03 | INFO | fairseq.tasks.translation | example reference: because we've UNKNOWNTOKENINREF the problem in chubut province, which is like a state in argentina where punta tombo is -- so that's about 1,000 UNKNOWNTOKENINREF of coastline -- but we haven't UNKNOWNTOKENINREF the problem in northern argentina, uruguay and UNKNOWNTOKENINREF.
2022-04-23 20:54:04 | INFO | fairseq.tasks.translation | example hypothesis: and you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know,
2022-04-23 20:54:04 | INFO | fairseq.tasks.translation | example reference: animal fan nellie mckay sings a sparkling UNKNOWNTOKENINREF te to her dear dog. she suggests we all do the same: "just go right to the pound / and find yourself a hound / and make that doggie proud / 'cause that's what it's all about."
2022-04-23 20:54:05 | INFO | fairseq.tasks.translation | example hypothesis: so, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP
2022-04-23 20:54:05 | INFO | fairseq.tasks.translation | example reference: with streams and rivers drying up because of over-usage, rob harmon has UNKNOWNTOKENINREF mented an ingenious market mechanism to bring back the water. farmers and beer companies find their fates intertwined in the intriguing centurUNKNOWNTOKENINREF old tale of prickly pear creek. & lt; em & gt; & lt; / em & gt;
2022-04-23 20:54:06 | INFO | fairseq.tasks.translation | example hypothesis: and that's not not not not not, and that we don't know, and you know, and you know, and you can't know, and you know, and you know, and you know, that's not not not not not not not not not not not not not not not not not not not not not not not, and the UNKNOWNTOKENINHYP, and the UNKNOWNTOKENINHYP, and the UNKNOWNTOKENINHYP, and they can't know, and you know, and you know, and you know, and you know, that
2022-04-23 20:54:06 | INFO | fairseq.tasks.translation | example reference: and the reality of the society that we're in is there are thousands and thousands of people out there leading lives of quiet, screaming desperation, where they work long, hard hours at jobs they hate to enable them to buy things they don't need to impress people they don't like.
2022-04-23 20:54:08 | INFO | fairseq.tasks.translation | example hypothesis: and i think, the world, i think, the world, and the world, the world, and the world, the world, and the world, the world, and the world, the world, and the world, and the world, the world, and the world, the world, the world, the world, the world, and the world, and i think, and the world, and i think, the world, and the world, the world is the world, and the world, and the world, the world, and the world,
2022-04-23 20:54:08 | INFO | fairseq.tasks.translation | example reference: and i organize it. and, well, it's also a bit different because an UNKNOWNTOKENINREF versus, let's say, a dance company finally is a negotiation between one's private world, one's UNKNOWNTOKENINREF tual world, the world of ideas, the world of aspirations, of UNKNOWNTOKENINREF tions, with the relationship of the exterior world and all the limitations, the naysayers.
2022-04-23 20:54:09 | INFO | fairseq.tasks.translation | example hypothesis: and we think that that we're going to do that that we're going to do that we're going to do that we're going to do, but we're going to do, but we're going to do, but we're going to do, but we're going to do, but we're going to do, but we're going to do, but we're going to do that, and we're going to do, but we're going to do, but we're going to do that, and we're going to do that, but that, but we're going to do that,
2022-04-23 20:54:09 | INFO | fairseq.tasks.translation | example reference: and i also hope that you share the idea that if engineers and UNKNOWNTOKENINREF can use all these different climatic parameters, it will be possible to create really good and comfortable outdoor conditions, to change our thermal perception that we feel comfortable in an outdoor environment, and we can do that with the best passive design, but also using the energy source of the site in qatar which is the sun.
2022-04-23 20:54:10 | INFO | fairseq.tasks.translation | example hypothesis: and i think, i think, i think, but i think, i think, but i think, but i think, but i think, i think, i think, i think, i think, i think, i think, but i think, i think, i think, i think, i think, i think, but i think, but i think, but i think, i think, i think, i think, i think, that i think, i think, i think, that i think, i think that i think, i think, and i think, i think, that i think, and i think, and i think, i think, i think that i think, i
2022-04-23 20:54:10 | INFO | fairseq.tasks.translation | example reference: i used to say that these people saved me, but what i now know is they did something even more important in that they empowered me to save myself, and crucially, they helped me to understand something which i'd always UNKNOWNTOKENINREF pected: that my voices were a meaningful response to traumatic life events, particularly childhood events, and as such were not my enemies but a source of insight into solvable emotional problems.
2022-04-23 20:54:12 | INFO | fairseq.tasks.translation | example hypothesis: and i said, "this is," "" the UNKNOWNTOKENINHYP, "" "the UNKNOWNTOKENINHYP," this is, "it's a UNKNOWNTOKENINHYP," "the UNKNOWNTOKENINHYP," the UNKNOWNTOKENINHYP, "it's," the UNKNOWNTOKENINHYP, "you know," you know, "you know," you know, "you know," you know, "the UNKNOWNTOKENINHYP," the UNKNOWNTOKENINHYP, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," "the UNKNOWNTOKENINHYP," the UNKNOWNTOKENINHYP, "you know," the UNKNOWNTOKENINHYP, "the UNKNOWNTOKENINHYP," the UNKNOWNTOKENINHYP, "" the UNKNOWNTOKENINHYP, "you know," you know, "the UNKNOWNTOKENINHYP," you know, "the UNKNOWNTOKENINHYP," the UNKNOWNTOKENINHYP, "the UNKNOWNTOKENINHYP," the UNKNOWNTOKENINHYP, "" the UNKNOWNTOKENINHYP, ""
2022-04-23 20:54:12 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at UNKNOWNTOKENINREF women is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-04-23 20:54:13 | INFO | fairseq.tasks.translation | example hypothesis: so, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, that we can be the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, that we can be the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP, the UNKNOWNTOKENINHYP,
2022-04-23 20:54:13 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of UNKNOWNTOKENINREF, and a lot of the design work that we're the most proud of with the aircraft came out of UNKNOWNTOKENINREF the unique problems of operating it on the ground -- everything from a continuouslUNKNOWNTOKENINREF variable transmission and liquiUNKNOWNTOKENINREF based cooling system that allows us to use an aircraft engine in stop-UNKNOWNTOKENINREF go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wUNKNOWNTOKENINREF folding mechanism that we'll see in a moment, to crash safety features.
2022-04-23 20:54:13 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.513 | nll_loss 7.37 | ppl 165.37 | bleu 1.27 | wps 2963.6 | wpb 2835.3 | bsz 115.6 | num_updates 1102
2022-04-23 20:54:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1102 updates
2022-04-23 20:54:13 | INFO | fairseq.trainer | Saving checkpoint to /root/cs4650-final-project/checkpoints/checkpoint1.pt
2022-04-23 20:54:14 | INFO | fairseq.trainer | Finished saving checkpoint to /root/cs4650-final-project/checkpoints/checkpoint1.pt
2022-04-23 20:54:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint1.pt (epoch 1 @ 1102 updates, score 1.27) (writing took 1.4556673779152334 seconds)
2022-04-23 20:54:14 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-04-23 20:54:14 | INFO | train | epoch 001 | loss 10.189 | nll_loss 9.354 | ppl 654.41 | wps 17715 | ups 4.94 | wpb 3583.6 | bsz 145.4 | num_updates 1102 | lr 0.00013775 | gnorm 1.86 | train_wall 160 | gb_free 12.1 | wall 224
2022-04-23 20:54:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2022-04-23 20:54:14 | INFO | fairseq.trainer | begin training epoch 2
2022-04-23 20:54:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-04-23 20:54:29 | INFO | train_inner | epoch 002:     98 / 1102 loss=8.498, nll_loss=7.396, ppl=168.43, wps=4773.9, ups=1.31, wpb=3647.1, bsz=162.7, num_updates=1200, lr=0.00015, gnorm=1.616, train_wall=14, gb_free=11.7, wall=238
2022-04-23 20:54:43 | INFO | train_inner | epoch 002:    198 / 1102 loss=8.488, nll_loss=7.386, ppl=167.31, wps=24652.5, ups=6.88, wpb=3585.3, bsz=143.4, num_updates=1300, lr=0.0001625, gnorm=1.536, train_wall=14, gb_free=12, wall=252
2022-04-23 20:54:58 | INFO | train_inner | epoch 002:    298 / 1102 loss=8.379, nll_loss=7.263, ppl=153.62, wps=24239.4, ups=6.86, wpb=3535.2, bsz=156.3, num_updates=1400, lr=0.000175, gnorm=1.613, train_wall=14, gb_free=11.9, wall=267
2022-04-23 20:55:12 | INFO | train_inner | epoch 002:    398 / 1102 loss=8.365, nll_loss=7.247, ppl=151.94, wps=24469.7, ups=6.85, wpb=3571.4, bsz=133.5, num_updates=1500, lr=0.0001875, gnorm=1.247, train_wall=14, gb_free=11.9, wall=282
2022-04-23 20:55:27 | INFO | train_inner | epoch 002:    498 / 1102 loss=8.239, nll_loss=7.107, ppl=137.81, wps=24690.9, ups=6.85, wpb=3605.1, bsz=141, num_updates=1600, lr=0.0002, gnorm=1.344, train_wall=14, gb_free=11.8, wall=296
2022-04-23 20:55:42 | INFO | train_inner | epoch 002:    598 / 1102 loss=8.168, nll_loss=7.028, ppl=130.51, wps=24620.7, ups=6.85, wpb=3594.6, bsz=147, num_updates=1700, lr=0.0002125, gnorm=1.335, train_wall=14, gb_free=12, wall=311
2022-04-23 20:55:56 | INFO | train_inner | epoch 002:    698 / 1102 loss=8.037, nll_loss=6.88, ppl=117.82, wps=24235.2, ups=6.84, wpb=3545.2, bsz=151.7, num_updates=1800, lr=0.000225, gnorm=1.333, train_wall=15, gb_free=12, wall=325
2022-04-23 20:56:11 | INFO | train_inner | epoch 002:    798 / 1102 loss=7.983, nll_loss=6.82, ppl=113.01, wps=24628.3, ups=6.84, wpb=3598.2, bsz=145.5, num_updates=1900, lr=0.0002375, gnorm=1.28, train_wall=14, gb_free=12, wall=340
2022-04-23 20:56:25 | INFO | train_inner | epoch 002:    898 / 1102 loss=7.958, nll_loss=6.792, ppl=110.79, wps=24684.5, ups=6.83, wpb=3612.1, bsz=136.8, num_updates=2000, lr=0.00025, gnorm=1.295, train_wall=15, gb_free=11.9, wall=355
2022-04-23 20:56:40 | INFO | train_inner | epoch 002:    998 / 1102 loss=7.859, nll_loss=6.681, ppl=102.63, wps=24206.3, ups=6.88, wpb=3517, bsz=142.6, num_updates=2100, lr=0.0002625, gnorm=1.321, train_wall=14, gb_free=11.9, wall=369
2022-04-23 20:56:55 | INFO | train_inner | epoch 002:   1098 / 1102 loss=7.799, nll_loss=6.613, ppl=97.92, wps=24973.5, ups=6.9, wpb=3621.5, bsz=140.6, num_updates=2200, lr=0.000275, gnorm=1.275, train_wall=14, gb_free=11.8, wall=384
2022-04-23 20:56:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-04-23 20:56:56 | INFO | fairseq.tasks.translation | example hypothesis: they're not going to do it.
2022-04-23 20:56:56 | INFO | fairseq.tasks.translation | example reference: they're just not moving.
2022-04-23 20:56:56 | INFO | fairseq.tasks.translation | example hypothesis: and then i said, "what is?"
2022-04-23 20:56:56 | INFO | fairseq.tasks.translation | example reference: i was like "what?"
2022-04-23 20:56:57 | INFO | fairseq.tasks.translation | example hypothesis: this is all of all?
2022-04-23 20:56:57 | INFO | fairseq.tasks.translation | example reference: is it all worked out? no.
2022-04-23 20:56:58 | INFO | fairseq.tasks.translation | example hypothesis: we need to need to do more more.
2022-04-23 20:56:58 | INFO | fairseq.tasks.translation | example reference: we need to use less to make more.
2022-04-23 20:56:58 | INFO | fairseq.tasks.translation | example hypothesis: but UNKNOWNTOKENINHYP: UNKNOWNTOKENINHYP.
2022-04-23 20:56:58 | INFO | fairseq.tasks.translation | example reference: but homage to singUNKNOWNTOKENINREF re.
2022-04-23 20:56:59 | INFO | fairseq.tasks.translation | example hypothesis: we call the UNKNOWNTOKENINHYP. "
2022-04-23 20:56:59 | INFO | fairseq.tasks.translation | example reference: we call this UNKNOWNTOKENINREF tational lensing.
2022-04-23 20:56:59 | INFO | fairseq.tasks.translation | example hypothesis: now, you think about UNKNOWNTOKENINHYP.
2022-04-23 20:56:59 | INFO | fairseq.tasks.translation | example reference: well, consider by UNKNOWNTOKENINREF y, the concept of physical health.
2022-04-23 20:57:00 | INFO | fairseq.tasks.translation | example hypothesis: these are the UNKNOWNTOKENINHYP.
2022-04-23 20:57:00 | INFO | fairseq.tasks.translation | example reference: these are the classic conditions that create regret.
2022-04-23 20:57:00 | INFO | fairseq.tasks.translation | example hypothesis: well, for example, it's good.
2022-04-23 20:57:00 | INFO | fairseq.tasks.translation | example reference: valence means good or bad, positive or negative.
2022-04-23 20:57:01 | INFO | fairseq.tasks.translation | example hypothesis: the fact is that it's UNKNOWNTOKENINHYP.
2022-04-23 20:57:01 | INFO | fairseq.tasks.translation | example reference: the bad news is they're rocket fuels.
2022-04-23 20:57:02 | INFO | fairseq.tasks.translation | example hypothesis: it can be able to be able to be able.
2022-04-23 20:57:02 | INFO | fairseq.tasks.translation | example reference: it can be used for calculating UNKNOWNTOKENINREF equations of all different types.
2022-04-23 20:57:02 | INFO | fairseq.tasks.translation | example hypothesis: we should be going to be going to do that.
2022-04-23 20:57:02 | INFO | fairseq.tasks.translation | example reference: we should probably slow down, and that point of action is probably now.
2022-04-23 20:57:03 | INFO | fairseq.tasks.translation | example hypothesis: so we went to the UNKNOWNTOKENINHYP.
2022-04-23 20:57:03 | INFO | fairseq.tasks.translation | example reference: decided to use UNKNOWNTOKENINREF ed content from cement and steel manufacturing.
2022-04-23 20:57:04 | INFO | fairseq.tasks.translation | example hypothesis: so, in fact, we've got two years ago.
2022-04-23 20:57:04 | INFO | fairseq.tasks.translation | example reference: just in the last two days, we got the new temperature UNKNOWNTOKENINREF ds in UNKNOWNTOKENINREF ary.
2022-04-23 20:57:04 | INFO | fairseq.tasks.translation | example hypothesis: and for example, this is something that it has been a lot of UNKNOWNTOKENINHYP.
2022-04-23 20:57:04 | INFO | fairseq.tasks.translation | example reference: and to remind you that here is an example in which architecture actually did something.
2022-04-23 20:57:05 | INFO | fairseq.tasks.translation | example hypothesis: these had had had had had had had had to be UNKNOWNTOKENINHYP.
2022-04-23 20:57:05 | INFO | fairseq.tasks.translation | example reference: and the grandmother had never let any westerners ever see her.
2022-04-23 20:57:05 | INFO | fairseq.tasks.translation | example hypothesis: this is something that we don't know about a few years.
2022-04-23 20:57:05 | INFO | fairseq.tasks.translation | example reference: this is something we've actually known for a while, so it's not a real breakthrough.
2022-04-23 20:57:06 | INFO | fairseq.tasks.translation | example hypothesis: it's a lot of UNKNOWNTOKENINHYP.
2022-04-23 20:57:06 | INFO | fairseq.tasks.translation | example reference: it's a social awkwardness like you're a stranger in a foreign land.
2022-04-23 20:57:07 | INFO | fairseq.tasks.translation | example hypothesis: i had to have a lot of people who had to do it.
2022-04-23 20:57:07 | INFO | fairseq.tasks.translation | example reference: i had learned to read music by then, or slowly learning to read music.
2022-04-23 20:57:07 | INFO | fairseq.tasks.translation | example hypothesis: they said, "if we were going to do if we don't know if we don't have a UNKNOWNTOKENINHYP.
2022-04-23 20:57:07 | INFO | fairseq.tasks.translation | example reference: they said they were going to beat us up if we didn't make some gunpowder for them.
2022-04-23 20:57:08 | INFO | fairseq.tasks.translation | example hypothesis: well, what we're going to do is: UNKNOWNTOKENINHYP.
2022-04-23 20:57:08 | INFO | fairseq.tasks.translation | example reference: well, we collect data from satellites, from airplanes, from ground UNKNOWNTOKENINREF cles, from people.
2022-04-23 20:57:09 | INFO | fairseq.tasks.translation | example hypothesis: and one of the one of the UNKNOWNTOKENINHYP was the UNKNOWNTOKENINHYP with these things that were the UNKNOWNTOKENINHYP with these things.
2022-04-23 20:57:09 | INFO | fairseq.tasks.translation | example reference: and one of the UNKNOWNTOKENINREF things that happened was this act of actually connecting to all these people.
2022-04-23 20:57:09 | INFO | fairseq.tasks.translation | example hypothesis: i want to give you a story to tell you a story.
2022-04-23 20:57:09 | INFO | fairseq.tasks.translation | example reference: i want to share one story about an innovation called drip irriUNKNOWNTOKENINREF.
2022-04-23 20:57:10 | INFO | fairseq.tasks.translation | example hypothesis: this was his wife.
2022-04-23 20:57:10 | INFO | fairseq.tasks.translation | example reference: the man, his name was mahmoud, and the child, whose name was rafi, left.
2022-04-23 20:57:11 | INFO | fairseq.tasks.translation | example hypothesis: and for me, it's going to take me to see the UNKNOWNTOKENINHYP.
2022-04-23 20:57:11 | INFO | fairseq.tasks.translation | example reference: and that allows me to check my timer discretely, here, without bending my elbow.
2022-04-23 20:57:11 | INFO | fairseq.tasks.translation | example hypothesis: in our UNKNOWNTOKENINHYP, she came to us, and she came out of us.
2022-04-23 20:57:11 | INFO | fairseq.tasks.translation | example reference: she arrived at our reserve from a reserve east of us on her UNKNOWNTOKENINREF atory route.
2022-04-23 20:57:12 | INFO | fairseq.tasks.translation | example hypothesis: if we're going to do that, i'm going to think that we're going to be able to be able to be able to be able.
2022-04-23 20:57:12 | INFO | fairseq.tasks.translation | example reference: if we can make it through the next 150 years, i think that your great great grandchildren will forget all about malthus.
2022-04-23 20:57:13 | INFO | fairseq.tasks.translation | example hypothesis: the UNKNOWNTOKENINHYP of these UNKNOWNTOKENINHYP, and then then, this is a little bit of UNKNOWNTOKENINHYP, and then they're going to take them with our UNKNOWNTOKENINHYP, and how they're going to do it.
2022-04-23 20:57:13 | INFO | fairseq.tasks.translation | example reference: those are then selected by the UNKNOWNTOKENINREF m, reproduced with mutation and reUNKNOWNTOKENINREF ations to introduce sex as well.
2022-04-23 20:57:13 | INFO | fairseq.tasks.translation | example hypothesis: but it doesn't know how to be able to be able to be able to be able.
2022-04-23 20:57:13 | INFO | fairseq.tasks.translation | example reference: now, it just shouldn't take these sorts of efforts to find out where the money in deals like this went.
2022-04-23 20:57:14 | INFO | fairseq.tasks.translation | example hypothesis: if you don't don't want to do if you're going to do your own UNKNOWNTOKENINHYP.
2022-04-23 20:57:14 | INFO | fairseq.tasks.translation | example reference: when people don't take their pills, when people don't follow doctors' orders -- these are behavior problems.
2022-04-23 20:57:15 | INFO | fairseq.tasks.translation | example hypothesis: so, it's so many people like many people like many people in the world.
2022-04-23 20:57:15 | INFO | fairseq.tasks.translation | example reference: so it's pretty sad to find that UNKNOWNTOKENINREF es, like so many other UNKNOWNTOKENINREF around the world, are losing their habitats.
2022-04-23 20:57:15 | INFO | fairseq.tasks.translation | example hypothesis: we found a few years ago, and we've got a few years ago, and we've got a few years.
2022-04-23 20:57:15 | INFO | fairseq.tasks.translation | example reference: we discovered bluegrass a few years ago, and we fell in love with it. we hope you guys will too.
2022-04-23 20:57:16 | INFO | fairseq.tasks.translation | example hypothesis: so i've got to do that, what is that we need to do?
2022-04-23 20:57:16 | INFO | fairseq.tasks.translation | example reference: so i was trying to, you know, take the engineer's version: can we build a mechanical system in inorganic materials that will do the same thing?
2022-04-23 20:57:17 | INFO | fairseq.tasks.translation | example hypothesis: we've done with us with us, we've got to have a UNKNOWNTOKENINHYP.
2022-04-23 20:57:17 | INFO | fairseq.tasks.translation | example reference: we consult to the media about canopy questions; we have a canopy newsletter; we have an email listserv.
2022-04-23 20:57:18 | INFO | fairseq.tasks.translation | example hypothesis: today today today today today today today today today today today today today today today today today today today today today, it's a lot of UNKNOWNTOKENINHYP with UNKNOWNTOKENINHYP and UNKNOWNTOKENINHYP.
2022-04-23 20:57:18 | INFO | fairseq.tasks.translation | example reference: what my purpose of the talk today really is, is to sort of indelibly scar your minds with these charismatic and majestic UNKNOWNTOKENINREF.
2022-04-23 20:57:18 | INFO | fairseq.tasks.translation | example hypothesis: so it's not going to do that if you're going to get the UNKNOWNTOKENINHYP?
2022-04-23 20:57:18 | INFO | fairseq.tasks.translation | example reference: now, it's no surprise that when you add consecutive fibonacci numbers, you get the next fibonacci number. right?
2022-04-23 20:57:19 | INFO | fairseq.tasks.translation | example hypothesis: but also, we don't know what we're going to do is that we're going to do.
2022-04-23 20:57:19 | INFO | fairseq.tasks.translation | example reference: yet we dither, taking no action to deflect the UNKNOWNTOKENINREF id, even though the longer we wait, the more difficult and expensive it becomes. "
2022-04-23 20:57:20 | INFO | fairseq.tasks.translation | example hypothesis: he had he had had a UNKNOWNTOKENINHYP, and he had a UNKNOWNTOKENINHYP, which was what he came out to me.
2022-04-23 20:57:20 | INFO | fairseq.tasks.translation | example reference: he had this amazing series of hairs growing out of a mole on the left side of his face, which i'm told is very good luck.
2022-04-23 20:57:21 | INFO | fairseq.tasks.translation | example hypothesis: this is the first first of the first UNKNOWNTOKENINHYP, and the first UNKNOWNTOKENINHYP, a UNKNOWNTOKENINHYP, a UNKNOWNTOKENINHYP, and in a UNKNOWNTOKENINHYP, in a UNKNOWNTOKENINHYP, a UNKNOWNTOKENINHYP, in a UNKNOWNTOKENINHYP, in a UNKNOWNTOKENINHYP, in a UNKNOWNTOKENINHYP, a UNKNOWNTOKENINHYP, in a UNKNOWNTOKENINHYP, a UNKNOWNTOKENINHYP,
2022-04-23 20:57:21 | INFO | fairseq.tasks.translation | example reference: here's the first battery -- a stack of coins, zinc and silver, separated by cardboard soaked in brine.
2022-04-23 20:57:22 | INFO | fairseq.tasks.translation | example hypothesis: this is the UNKNOWNTOKENINHYP of the UNKNOWNTOKENINHYP in UNKNOWNTOKENINHYP, because people who are in the united states, because there are a lot of people in the world.
2022-04-23 20:57:22 | INFO | fairseq.tasks.translation | example reference: durkheim called this level the level of the sacred because he believed that the function of religion was to unite people into a group, into a moral community.
2022-04-23 20:57:22 | INFO | fairseq.tasks.translation | example hypothesis: but what i do is that a little bit of UNKNOWNTOKENINHYP, and when i'm going to look at the UNKNOWNTOKENINHYP.
2022-04-23 20:57:22 | INFO | fairseq.tasks.translation | example reference: but the UNKNOWNTOKENINREF match up if i rotate by a sixth of a turn around the point where all the trianUNKNOWNTOKENINREF meet.
2022-04-23 20:57:23 | INFO | fairseq.tasks.translation | example hypothesis: there are some of them, and then you're going to do with them, and then i'm going to give them them them them them with them with them with them, and then i'm going to do them with them with them.
2022-04-23 20:57:23 | INFO | fairseq.tasks.translation | example reference: there's little brochures all around outside, and if any of you have anything to do with children and care about their future, i beg that you pick up that brochure.
2022-04-23 20:57:24 | INFO | fairseq.tasks.translation | example hypothesis: and it's just just just just just just just four percent of the time that we're going to have a little bit of UNKNOWNTOKENINHYP, which is a little bit of UNKNOWNTOKENINHYP.
2022-04-23 20:57:24 | INFO | fairseq.tasks.translation | example reference: and this is only done in four hours, 50 times faster than the current state of the art, at a cost that will be five to 500 times cheaper than the current options.
2022-04-23 20:57:25 | INFO | fairseq.tasks.translation | example hypothesis: so, what i'm going to do is that you're going to do with UNKNOWNTOKENINHYP, and i'm going to talk about what you can do with UNKNOWNTOKENINHYP, and you can do with UNKNOWNTOKENINHYP, and you can do with the UNKNOWNTOKENINHYP, and you can see what you can do with the UNKNOWNTOKENINHYP, and you can
2022-04-23 20:57:25 | INFO | fairseq.tasks.translation | example reference: i'm fascinated with the idea of what happens when you merge UNKNOWNTOKENINREF with technology, and i remember reading about this idea of being able to reprogram UNKNOWNTOKENINREF, in the future, away from disease and aging.
2022-04-23 20:57:26 | INFO | fairseq.tasks.translation | example hypothesis: but the problem that the problem is that they're going to do it in UNKNOWNTOKENINHYP.
2022-04-23 20:57:26 | INFO | fairseq.tasks.translation | example reference: but the problem with relying on rules and inUNKNOWNTOKENINREF ves is that they demorUNKNOWNTOKENINREF professional activity, and they demorUNKNOWNTOKENINREF professional activity in two UNKNOWNTOKENINREF.
2022-04-23 20:57:27 | INFO | fairseq.tasks.translation | example hypothesis: i'm going to say, "i'm going to say," we have a few years ago, "we've got a few years ago, and we've been in UNKNOWNTOKENINHYP."
2022-04-23 20:57:27 | INFO | fairseq.tasks.translation | example reference: i belong to an internet discussion forum, an UNKNOWNTOKENINREF internet discussion forum, and i asked them, i said, "since 1960, we've had exactly 204 UNKNOWNTOKENINREF heads of state, since 1960."
2022-04-23 20:57:28 | INFO | fairseq.tasks.translation | example hypothesis: and when i came to me to me, and i was very important, and i really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really really important to
2022-04-23 20:57:28 | INFO | fairseq.tasks.translation | example reference: and as these dark clouds were circling me, and i was finding it really, really difficult to think of anything good, i said to myself that i really needed a way to focus on the positive somehow.
2022-04-23 20:57:28 | INFO | fairseq.tasks.translation | example hypothesis: and for example, for example, for example, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know,
2022-04-23 20:57:28 | INFO | fairseq.tasks.translation | example reference: for example, gUNKNOWNTOKENINREF age discharge, something you would think just simply goes away, but the laws regulating ship discharge of gUNKNOWNTOKENINREF age actually get weaker the further you are from shore.
2022-04-23 20:57:29 | INFO | fairseq.tasks.translation | example hypothesis: he went to a UNKNOWNTOKENINHYP, and then he would have a little bit of UNKNOWNTOKENINHYP, and then he would have a little bit of UNKNOWNTOKENINHYP, and then then he would have a little bit of UNKNOWNTOKENINHYP.
2022-04-23 20:57:29 | INFO | fairseq.tasks.translation | example reference: see, he owned a UNKNOWNTOKENINREF mium plating company, and they had to move heavy steel parts between tanks of chemicals, and so he needed an industrial robot like this that could basically do the heavy lifting.
2022-04-23 20:57:30 | INFO | fairseq.tasks.translation | example hypothesis: and the idea of the UNKNOWNTOKENINHYP is that people who are going to do their own own UNKNOWNTOKENINHYP, their own own UNKNOWNTOKENINHYP, their own own own UNKNOWNTOKENINHYP, to their own own own UNKNOWNTOKENINHYP.
2022-04-23 20:57:30 | INFO | fairseq.tasks.translation | example reference: and the religious police imposes the supposed UNKNOWNTOKENINREF ic way of life on every citizen, by force -- like women are forced to cover their heads -- wear the hijab, the UNKNOWNTOKENINREF ic head cover.
2022-04-23 20:57:31 | INFO | fairseq.tasks.translation | example hypothesis: they're going to go with the world of the world, and the world is the world of the world, the world of the world, the world, the world, and the world of the world, the world, the world of the world, the world.
2022-04-23 20:57:31 | INFO | fairseq.tasks.translation | example reference: come with me to the bottom of the world, antUNKNOWNTOKENINREF tica, the highest, driest, windiest, and yes, coldest region on earth -- more arid than the UNKNOWNTOKENINREF a and, in parts, colder than UNKNOWNTOKENINREF.
2022-04-23 20:57:32 | INFO | fairseq.tasks.translation | example hypothesis: you see here, you can see here, he's called the UNKNOWNTOKENINHYP, he's UNKNOWNTOKENINHYP, and he's going to go up to the UNKNOWNTOKENINHYP, and he's going to the UNKNOWNTOKENINHYP, he's going to see how he's going to show you.
2022-04-23 20:57:32 | INFO | fairseq.tasks.translation | example reference: that's kellar autumn, my former ph.d. student, professor now at lewis and clark, literally giving his firUNKNOWNTOKENINREF born child up for this test.
2022-04-23 20:57:33 | INFO | fairseq.tasks.translation | example hypothesis: it's a lot of us that we're going to think that we're going to think of the UNKNOWNTOKENINHYP, and that's a lot of the most important thing that we're going to think of us.
2022-04-23 20:57:33 | INFO | fairseq.tasks.translation | example reference: however, while it's easier to think of us, the citizens, the police, the army, as the good guys, and them, the narcos, the carteles, as the bad guys, if you think about it, the latter are only providing a service to the former.
2022-04-23 20:57:34 | INFO | fairseq.tasks.translation | example hypothesis: and in the last year, in the last year, in the last year, i've been in UNKNOWNTOKENINHYP, and i'm going to talk about this project, because i'm going to say, "this is a few years ago, and i think this is a few years ago.
2022-04-23 20:57:34 | INFO | fairseq.tasks.translation | example reference: and for the following 25 years, living in italy, living in UNKNOWNTOKENINREF, i doled out a piece of this romance to anybody who'd pay for it -- this sense, this aesthetic feeling, for the experience revolving around a designed object.
2022-04-23 20:57:35 | INFO | fairseq.tasks.translation | example hypothesis: because because we have this problem, because we don't have a problem, but we have a problem that we have a problem in UNKNOWNTOKENINHYP, and we have a UNKNOWNTOKENINHYP, but we have a problem in the UNKNOWNTOKENINHYP, but we have a problem that we have a lot of UNKNOWNTOKENINHYP, and we have a problem.
2022-04-23 20:57:35 | INFO | fairseq.tasks.translation | example reference: because we've UNKNOWNTOKENINREF the problem in chubut province, which is like a state in argentina where punta tombo is -- so that's about 1,000 UNKNOWNTOKENINREF of coastline -- but we haven't UNKNOWNTOKENINREF the problem in northern argentina, uruguay and UNKNOWNTOKENINREF.
2022-04-23 20:57:36 | INFO | fairseq.tasks.translation | example hypothesis: for example, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, "we're going to do it, and you're going to do it, and you know," you know, you know, you know, "you know, it's a little bit of what you're going to do."
2022-04-23 20:57:36 | INFO | fairseq.tasks.translation | example reference: animal fan nellie mckay sings a sparkling UNKNOWNTOKENINREF te to her dear dog. she suggests we all do the same: "just go right to the pound / and find yourself a hound / and make that doggie proud / 'cause that's what it's all about."
2022-04-23 20:57:37 | INFO | fairseq.tasks.translation | example hypothesis: and in UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP, UNKNOWNTOKENINHYP
2022-04-23 20:57:37 | INFO | fairseq.tasks.translation | example reference: with streams and rivers drying up because of over-usage, rob harmon has UNKNOWNTOKENINREF mented an ingenious market mechanism to bring back the water. farmers and beer companies find their fates intertwined in the intriguing centurUNKNOWNTOKENINREF old tale of prickly pear creek. & lt; em & gt; & lt; / em & gt;
2022-04-23 20:57:38 | INFO | fairseq.tasks.translation | example hypothesis: and the idea of the UNKNOWNTOKENINHYP is that we're going to live in life, and there are a lot of life that we're going to live in their life, because there are a lot of life.
2022-04-23 20:57:38 | INFO | fairseq.tasks.translation | example reference: and the reality of the society that we're in is there are thousands and thousands of people out there leading lives of quiet, screaming desperation, where they work long, hard hours at jobs they hate to enable them to buy things they don't need to impress people they don't like.
2022-04-23 20:57:39 | INFO | fairseq.tasks.translation | example hypothesis: and i'm going to tell you, "well, you know, in the world, the world, the world, the world, the world of the world, the world, the world, the world, and the world is the world of the world of the world, and the world of the world, the world, and the world of the world, the world of the world.
2022-04-23 20:57:39 | INFO | fairseq.tasks.translation | example reference: and i organize it. and, well, it's also a bit different because an UNKNOWNTOKENINREF versus, let's say, a dance company finally is a negotiation between one's private world, one's UNKNOWNTOKENINREF tual world, the world of ideas, the world of aspirations, of UNKNOWNTOKENINREF tions, with the relationship of the exterior world and all the limitations, the naysayers.
2022-04-23 20:57:40 | INFO | fairseq.tasks.translation | example hypothesis: and they're also also also also also important for us, but in fact, we're going to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able
2022-04-23 20:57:40 | INFO | fairseq.tasks.translation | example reference: and i also hope that you share the idea that if engineers and UNKNOWNTOKENINREF can use all these different climatic parameters, it will be possible to create really good and comfortable outdoor conditions, to change our thermal perception that we feel comfortable in an outdoor environment, and we can do that with the best passive design, but also using the energy source of the site in qatar which is the sun.
2022-04-23 20:57:42 | INFO | fairseq.tasks.translation | example hypothesis: and i said, i said, "well, well, i've got a lot of UNKNOWNTOKENINHYP, but i'm going to have a lot of UNKNOWNTOKENINHYP, and i don't know that they were going to have a little bit of UNKNOWNTOKENINHYP, but i'd like to do it.
2022-04-23 20:57:42 | INFO | fairseq.tasks.translation | example reference: i used to say that these people saved me, but what i now know is they did something even more important in that they empowered me to save myself, and crucially, they helped me to understand something which i'd always UNKNOWNTOKENINREF pected: that my voices were a meaningful response to traumatic life events, particularly childhood events, and as such were not my enemies but a source of insight into solvable emotional problems.
2022-04-23 20:57:43 | INFO | fairseq.tasks.translation | example hypothesis: UNKNOWNTOKENINHYP: one of the world said, "you know," well, "you know, you know, you know, you know," you know, you know, you know, you know, "you know, you know, you know, you know, you know," you know, you know, you know, "you know, you know, you know," you know, "you know," you know, "you know, you know, you know, you know," you know, "you know," you know, "you know," you know, you know, "you know," you know, you know, "you know," you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, "
2022-04-23 20:57:43 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at UNKNOWNTOKENINREF women is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-04-23 20:57:44 | INFO | fairseq.tasks.translation | example hypothesis: UNKNOWNTOKENINHYP is the UNKNOWNTOKENINHYP of the UNKNOWNTOKENINHYP that we're going to do that we're going to do a little bit of the UNKNOWNTOKENINHYP, and we're going to do that we were going to have a little bit of the way that we were going to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to
2022-04-23 20:57:44 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of UNKNOWNTOKENINREF, and a lot of the design work that we're the most proud of with the aircraft came out of UNKNOWNTOKENINREF the unique problems of operating it on the ground -- everything from a continuouslUNKNOWNTOKENINREF variable transmission and liquiUNKNOWNTOKENINREF based cooling system that allows us to use an aircraft engine in stop-UNKNOWNTOKENINREF go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wUNKNOWNTOKENINREF folding mechanism that we'll see in a moment, to crash safety features.
2022-04-23 20:57:44 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.481 | nll_loss 6.207 | ppl 73.9 | bleu 4.66 | wps 3655.1 | wpb 2835.3 | bsz 115.6 | num_updates 2204 | best_bleu 4.66
2022-04-23 20:57:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2204 updates
2022-04-23 20:57:44 | INFO | fairseq.trainer | Saving checkpoint to /root/cs4650-final-project/checkpoints/checkpoint2.pt
2022-04-23 20:57:45 | INFO | fairseq.trainer | Finished saving checkpoint to /root/cs4650-final-project/checkpoints/checkpoint2.pt
2022-04-23 20:57:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint2.pt (epoch 2 @ 2204 updates, score 4.66) (writing took 1.9425157560035586 seconds)
2022-04-23 20:57:46 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-04-23 20:57:46 | INFO | train | epoch 002 | loss 8.16 | nll_loss 7.019 | ppl 129.66 | wps 18671.1 | ups 5.21 | wpb 3583.6 | bsz 145.4 | num_updates 2204 | lr 0.0002755 | gnorm 1.38 | train_wall 159 | gb_free 11.9 | wall 435
2022-04-23 20:57:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2022-04-23 20:57:46 | INFO | fairseq.trainer | begin training epoch 3
2022-04-23 20:57:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-04-23 20:58:00 | INFO | train_inner | epoch 003:     96 / 1102 loss=7.677, nll_loss=6.477, ppl=89.1, wps=5514.3, ups=1.53, wpb=3598.2, bsz=139, num_updates=2300, lr=0.0002875, gnorm=1.243, train_wall=14, gb_free=12.1, wall=449
2022-04-23 20:58:14 | INFO | train_inner | epoch 003:    196 / 1102 loss=7.636, nll_loss=6.433, ppl=86.38, wps=24771.9, ups=6.98, wpb=3550.9, bsz=140.2, num_updates=2400, lr=0.0003, gnorm=1.316, train_wall=14, gb_free=12, wall=463
2022-04-23 20:58:29 | INFO | train_inner | epoch 003:    296 / 1102 loss=7.529, nll_loss=6.311, ppl=79.4, wps=24943.7, ups=6.93, wpb=3598, bsz=144, num_updates=2500, lr=0.0003125, gnorm=1.281, train_wall=14, gb_free=11.8, wall=478
2022-04-23 20:58:43 | INFO | train_inner | epoch 003:    396 / 1102 loss=7.387, nll_loss=6.152, ppl=71.11, wps=24905.7, ups=6.89, wpb=3612.2, bsz=145.7, num_updates=2600, lr=0.000325, gnorm=1.267, train_wall=14, gb_free=11.9, wall=492
2022-04-23 20:58:58 | INFO | train_inner | epoch 003:    496 / 1102 loss=7.419, nll_loss=6.188, ppl=72.92, wps=24838.6, ups=6.89, wpb=3607, bsz=134.4, num_updates=2700, lr=0.0003375, gnorm=1.274, train_wall=14, gb_free=12.7, wall=507
2022-04-23 20:59:12 | INFO | train_inner | epoch 003:    596 / 1102 loss=7.231, nll_loss=5.977, ppl=63, wps=24115.9, ups=6.86, wpb=3514.6, bsz=151.8, num_updates=2800, lr=0.00035, gnorm=1.317, train_wall=14, gb_free=11.8, wall=521
Traceback (most recent call last):
  File "/opt/conda/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/root/cs4650-final-project/fairseq/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/root/cs4650-final-project/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/root/cs4650-final-project/fairseq/fairseq_cli/train.py", line 190, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/opt/conda/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/root/cs4650-final-project/fairseq/fairseq_cli/train.py", line 316, in train
    log_output = trainer.train_step(samples)
  File "/opt/conda/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/root/cs4650-final-project/fairseq/fairseq/trainer.py", line 832, in train_step
    del loss
KeyboardInterrupt
